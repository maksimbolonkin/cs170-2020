{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 100\n",
    "lr_init = 0.1\n",
    "batch_size = 32\n",
    "lr_change=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(16, 32, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(7*7*128, 512), nn.ReLU(),\n",
    "            nn.Linear(512, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(-1, 7*7*128)\n",
    "        x = self.clf(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "train_dataset = datasets.ImageFolder('data_files/dogs-vs-cats/train',\n",
    "                                    transforms.Compose([\n",
    "                                        transforms.RandomResizedCrop(224),\n",
    "                                        transforms.RandomHorizontalFlip(),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                           std=[0.229, 0.224, 0.225])\n",
    "                                    ]))\n",
    "val_dataset = datasets.ImageFolder('data_files/dogs-vs-cats/val',\n",
    "                                    transforms.Compose([\n",
    "                                        transforms.Resize(256),\n",
    "                                        transforms.CenterCrop(224),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                           std=[0.229, 0.224, 0.225])\n",
    "                                    ]))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train (func)\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    \n",
    "    losses = []\n",
    "    batch_num = len(train_loader)\n",
    "    \n",
    "    for i, (images, targets) in enumerate(train_loader):\n",
    "        output = model(images)\n",
    "        loss = criterion(output, targets)\n",
    "        \n",
    "        print('Epoch {}, batch {}/{}, train loss={}'.format(epoch, i, batch_num, loss.item()))\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch {}, avg train loss={}'.format(epoch, np.mean(losses)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval (func)\n",
    "def validate(epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    losses = []\n",
    "    batch_num = len(val_loader)\n",
    "    \n",
    "    for i, (images, targets) in enumerate(val_loader):\n",
    "        output = model(images)\n",
    "        loss = criterion(output, targets)\n",
    "        \n",
    "        print('Epoch {}, batch {}/{}, val loss={}'.format(epoch, i, batch_num, loss.item()))\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    print('=== Epoch {}, avg val loss={} ==='.format(epoch, np.mean(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe control learning rate schedule\n",
    "def change_lr(epoch):\n",
    "    lr = lr_init * (0.1 ** (epoch//lr_change))\n",
    "    for params in optimizer.param_groups:\n",
    "        params['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network object, create loss function, optimizer\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr_init, momentum=0.9, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, batch 0/625, train loss=0.6956437230110168\n",
      "Epoch 0, batch 1/625, train loss=0.6946680545806885\n",
      "Epoch 0, batch 2/625, train loss=0.6916537284851074\n",
      "Epoch 0, batch 3/625, train loss=0.7081995010375977\n",
      "Epoch 0, batch 4/625, train loss=0.704748809337616\n",
      "Epoch 0, batch 5/625, train loss=0.694390594959259\n",
      "Epoch 0, batch 6/625, train loss=0.6867038011550903\n",
      "Epoch 0, batch 7/625, train loss=0.6997222900390625\n",
      "Epoch 0, batch 8/625, train loss=0.6915119886398315\n",
      "Epoch 0, batch 9/625, train loss=0.6929004788398743\n",
      "Epoch 0, batch 10/625, train loss=0.7121329307556152\n",
      "Epoch 0, batch 11/625, train loss=0.6946157813072205\n",
      "Epoch 0, batch 12/625, train loss=0.7223030924797058\n",
      "Epoch 0, batch 13/625, train loss=0.7180026173591614\n",
      "Epoch 0, batch 14/625, train loss=0.710652768611908\n",
      "Epoch 0, batch 15/625, train loss=0.6911607980728149\n",
      "Epoch 0, batch 16/625, train loss=0.6934406757354736\n",
      "Epoch 0, batch 17/625, train loss=0.6916505694389343\n",
      "Epoch 0, batch 18/625, train loss=0.7008422613143921\n",
      "Epoch 0, batch 19/625, train loss=0.6760181188583374\n",
      "Epoch 0, batch 20/625, train loss=0.6927032470703125\n",
      "Epoch 0, batch 21/625, train loss=0.6669068336486816\n",
      "Epoch 0, batch 22/625, train loss=0.7320865988731384\n",
      "Epoch 0, batch 23/625, train loss=0.6391937732696533\n",
      "Epoch 0, batch 24/625, train loss=0.7158316969871521\n",
      "Epoch 0, batch 25/625, train loss=0.7725299000740051\n",
      "Epoch 0, batch 26/625, train loss=0.7511845827102661\n",
      "Epoch 0, batch 27/625, train loss=0.7167484760284424\n",
      "Epoch 0, batch 28/625, train loss=0.6927204728126526\n",
      "Epoch 0, batch 29/625, train loss=0.6773696541786194\n",
      "Epoch 0, batch 30/625, train loss=0.6886454820632935\n",
      "Epoch 0, batch 31/625, train loss=0.6933469772338867\n",
      "Epoch 0, batch 32/625, train loss=0.6926209926605225\n",
      "Epoch 0, batch 33/625, train loss=0.7075562477111816\n",
      "Epoch 0, batch 34/625, train loss=0.691194474697113\n",
      "Epoch 0, batch 35/625, train loss=0.6913398504257202\n",
      "Epoch 0, batch 36/625, train loss=0.7389481067657471\n",
      "Epoch 0, batch 37/625, train loss=0.6963833570480347\n",
      "Epoch 0, batch 38/625, train loss=0.687213659286499\n",
      "Epoch 0, batch 39/625, train loss=0.6783494353294373\n",
      "Epoch 0, batch 40/625, train loss=0.6912283897399902\n",
      "Epoch 0, batch 41/625, train loss=0.698262631893158\n",
      "Epoch 0, batch 42/625, train loss=0.6912425756454468\n",
      "Epoch 0, batch 43/625, train loss=0.6881787776947021\n",
      "Epoch 0, batch 44/625, train loss=0.6912652850151062\n",
      "Epoch 0, batch 45/625, train loss=0.7075275778770447\n",
      "Epoch 0, batch 46/625, train loss=0.6962587833404541\n",
      "Epoch 0, batch 47/625, train loss=0.6882414817810059\n",
      "Epoch 0, batch 48/625, train loss=0.6922702789306641\n",
      "Epoch 0, batch 49/625, train loss=0.6941601634025574\n",
      "Epoch 0, batch 50/625, train loss=0.6926131248474121\n",
      "Epoch 0, batch 51/625, train loss=0.6922851204872131\n",
      "Epoch 0, batch 52/625, train loss=0.695293664932251\n",
      "Epoch 0, batch 53/625, train loss=0.6919754147529602\n",
      "Epoch 0, batch 54/625, train loss=0.6925029754638672\n",
      "Epoch 0, batch 55/625, train loss=0.68962562084198\n",
      "Epoch 0, batch 56/625, train loss=0.7072516083717346\n",
      "Epoch 0, batch 57/625, train loss=0.6954390406608582\n",
      "Epoch 0, batch 58/625, train loss=0.6933345198631287\n",
      "Epoch 0, batch 59/625, train loss=0.6943260431289673\n",
      "Epoch 0, batch 60/625, train loss=0.693170428276062\n",
      "Epoch 0, batch 61/625, train loss=0.6933653354644775\n",
      "Epoch 0, batch 62/625, train loss=0.6896176934242249\n",
      "Epoch 0, batch 63/625, train loss=0.6972390413284302\n",
      "Epoch 0, batch 64/625, train loss=0.6946805715560913\n",
      "Epoch 0, batch 65/625, train loss=0.691196858882904\n",
      "Epoch 0, batch 66/625, train loss=0.7071073651313782\n",
      "Epoch 0, batch 67/625, train loss=0.6912248730659485\n",
      "Epoch 0, batch 68/625, train loss=0.6942367553710938\n",
      "Epoch 0, batch 69/625, train loss=0.6938399076461792\n",
      "Epoch 0, batch 70/625, train loss=0.7069115042686462\n",
      "Epoch 0, batch 71/625, train loss=0.6943175792694092\n",
      "Epoch 0, batch 72/625, train loss=0.7110224366188049\n",
      "Epoch 0, batch 73/625, train loss=0.68211829662323\n",
      "Epoch 0, batch 74/625, train loss=0.6941326856613159\n",
      "Epoch 0, batch 75/625, train loss=0.6912252902984619\n",
      "Epoch 0, batch 76/625, train loss=0.6792054176330566\n",
      "Epoch 0, batch 77/625, train loss=0.7015871405601501\n",
      "Epoch 0, batch 78/625, train loss=0.7085232734680176\n",
      "Epoch 0, batch 79/625, train loss=0.6860399842262268\n",
      "Epoch 0, batch 80/625, train loss=0.6860764026641846\n",
      "Epoch 0, batch 81/625, train loss=0.713342010974884\n",
      "Epoch 0, batch 82/625, train loss=0.7052173018455505\n",
      "Epoch 0, batch 83/625, train loss=0.6978297233581543\n",
      "Epoch 0, batch 84/625, train loss=0.6969384551048279\n",
      "Epoch 0, batch 85/625, train loss=0.693743109703064\n",
      "Epoch 0, batch 86/625, train loss=0.6946831345558167\n",
      "Epoch 0, batch 87/625, train loss=0.6978042125701904\n",
      "Epoch 0, batch 88/625, train loss=0.6915538311004639\n",
      "Epoch 0, batch 89/625, train loss=0.6865271925926208\n",
      "Epoch 0, batch 90/625, train loss=0.7006887197494507\n",
      "Epoch 0, batch 91/625, train loss=0.6976481676101685\n",
      "Epoch 0, batch 92/625, train loss=0.6826707124710083\n",
      "Epoch 0, batch 93/625, train loss=0.7042194604873657\n",
      "Epoch 0, batch 94/625, train loss=0.6997804641723633\n",
      "Epoch 0, batch 95/625, train loss=0.699267566204071\n",
      "Epoch 0, batch 96/625, train loss=0.692043125629425\n",
      "Epoch 0, batch 97/625, train loss=0.691907525062561\n",
      "Epoch 0, batch 98/625, train loss=0.6952190399169922\n",
      "Epoch 0, batch 99/625, train loss=0.6888505220413208\n",
      "Epoch 0, batch 100/625, train loss=0.6912040710449219\n",
      "Epoch 0, batch 101/625, train loss=0.6866829991340637\n",
      "Epoch 0, batch 102/625, train loss=0.6971234679222107\n",
      "Epoch 0, batch 103/625, train loss=0.698067307472229\n",
      "Epoch 0, batch 104/625, train loss=0.7113781571388245\n",
      "Epoch 0, batch 105/625, train loss=0.6917263865470886\n",
      "Epoch 0, batch 106/625, train loss=0.6914770007133484\n",
      "Epoch 0, batch 107/625, train loss=0.6671627759933472\n",
      "Epoch 0, batch 108/625, train loss=0.6861814260482788\n",
      "Epoch 0, batch 109/625, train loss=0.6744364500045776\n",
      "Epoch 0, batch 110/625, train loss=0.7054666876792908\n",
      "Epoch 0, batch 111/625, train loss=0.7063010334968567\n",
      "Epoch 0, batch 112/625, train loss=0.6654248833656311\n",
      "Epoch 0, batch 113/625, train loss=0.7207744121551514\n",
      "Epoch 0, batch 114/625, train loss=0.6531449556350708\n",
      "Epoch 0, batch 115/625, train loss=0.6993780136108398\n",
      "Epoch 0, batch 116/625, train loss=0.692470908164978\n",
      "Epoch 0, batch 117/625, train loss=0.6994149088859558\n",
      "Epoch 0, batch 118/625, train loss=0.6855189800262451\n",
      "Epoch 0, batch 119/625, train loss=0.6543697118759155\n",
      "Epoch 0, batch 120/625, train loss=0.6924141049385071\n",
      "Epoch 0, batch 121/625, train loss=0.6928572654724121\n",
      "Epoch 0, batch 122/625, train loss=0.7243682742118835\n",
      "Epoch 0, batch 123/625, train loss=0.7278728485107422\n",
      "Epoch 0, batch 124/625, train loss=0.6808682680130005\n",
      "Epoch 0, batch 125/625, train loss=0.7071143388748169\n",
      "Epoch 0, batch 126/625, train loss=0.6896294951438904\n",
      "Epoch 0, batch 127/625, train loss=0.696444034576416\n",
      "Epoch 0, batch 128/625, train loss=0.6916660666465759\n",
      "Epoch 0, batch 129/625, train loss=0.699930727481842\n",
      "Epoch 0, batch 130/625, train loss=0.7100035548210144\n",
      "Epoch 0, batch 131/625, train loss=0.6646747589111328\n",
      "Epoch 0, batch 132/625, train loss=0.7434555292129517\n",
      "Epoch 0, batch 133/625, train loss=0.6934762597084045\n",
      "Epoch 0, batch 134/625, train loss=0.6931114196777344\n",
      "Epoch 0, batch 135/625, train loss=0.6708014011383057\n",
      "Epoch 0, batch 136/625, train loss=0.6998082995414734\n",
      "Epoch 0, batch 137/625, train loss=0.6514275670051575\n",
      "Epoch 0, batch 138/625, train loss=0.6778826713562012\n",
      "Epoch 0, batch 139/625, train loss=0.7347832918167114\n",
      "Epoch 0, batch 140/625, train loss=0.7085740566253662\n",
      "Epoch 0, batch 141/625, train loss=0.6989230513572693\n",
      "Epoch 0, batch 142/625, train loss=0.713321328163147\n",
      "Epoch 0, batch 143/625, train loss=0.6877685785293579\n",
      "Epoch 0, batch 144/625, train loss=0.6973147988319397\n",
      "Epoch 0, batch 145/625, train loss=0.6931474804878235\n",
      "Epoch 0, batch 146/625, train loss=0.6864470839500427\n",
      "Epoch 0, batch 147/625, train loss=0.6992271542549133\n",
      "Epoch 0, batch 148/625, train loss=0.6746718287467957\n",
      "Epoch 0, batch 149/625, train loss=0.6777411103248596\n",
      "Epoch 0, batch 150/625, train loss=0.7142853140830994\n",
      "Epoch 0, batch 151/625, train loss=0.7075147032737732\n",
      "Epoch 0, batch 152/625, train loss=0.7418864965438843\n",
      "Epoch 0, batch 153/625, train loss=0.6757634878158569\n",
      "Epoch 0, batch 154/625, train loss=0.7051302790641785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, batch 155/625, train loss=0.6853950619697571\n",
      "Epoch 0, batch 156/625, train loss=0.669978678226471\n",
      "Epoch 0, batch 157/625, train loss=0.728427529335022\n",
      "Epoch 0, batch 158/625, train loss=0.7306479215621948\n",
      "Epoch 0, batch 159/625, train loss=0.6913806796073914\n",
      "Epoch 0, batch 160/625, train loss=0.6928365230560303\n",
      "Epoch 0, batch 161/625, train loss=0.691790759563446\n",
      "Epoch 0, batch 162/625, train loss=0.7019097805023193\n",
      "Epoch 0, batch 163/625, train loss=0.6958920955657959\n",
      "Epoch 0, batch 164/625, train loss=0.6861026883125305\n",
      "Epoch 0, batch 165/625, train loss=0.6918165683746338\n",
      "Epoch 0, batch 166/625, train loss=0.6720803380012512\n",
      "Epoch 0, batch 167/625, train loss=0.6776589751243591\n",
      "Epoch 0, batch 168/625, train loss=0.7028496861457825\n",
      "Epoch 0, batch 169/625, train loss=0.70401930809021\n",
      "Epoch 0, batch 170/625, train loss=0.7040043473243713\n",
      "Epoch 0, batch 171/625, train loss=0.7379370927810669\n",
      "Epoch 0, batch 172/625, train loss=0.7280022501945496\n",
      "Epoch 0, batch 173/625, train loss=0.7136090397834778\n",
      "Epoch 0, batch 174/625, train loss=0.6922010779380798\n",
      "Epoch 0, batch 175/625, train loss=0.7025418281555176\n",
      "Epoch 0, batch 176/625, train loss=0.6979066133499146\n",
      "Epoch 0, batch 177/625, train loss=0.691217839717865\n",
      "Epoch 0, batch 178/625, train loss=0.7124112248420715\n",
      "Epoch 0, batch 179/625, train loss=0.696600615978241\n",
      "Epoch 0, batch 180/625, train loss=0.6815394759178162\n",
      "Epoch 0, batch 181/625, train loss=0.6962747573852539\n",
      "Epoch 0, batch 182/625, train loss=0.710267186164856\n",
      "Epoch 0, batch 183/625, train loss=0.6874570250511169\n",
      "Epoch 0, batch 184/625, train loss=0.6852242350578308\n",
      "Epoch 0, batch 185/625, train loss=0.688512921333313\n",
      "Epoch 0, batch 186/625, train loss=0.6971091628074646\n",
      "Epoch 0, batch 187/625, train loss=0.6966216564178467\n",
      "Epoch 0, batch 188/625, train loss=0.7057896852493286\n",
      "Epoch 0, batch 189/625, train loss=0.6933712363243103\n",
      "Epoch 0, batch 190/625, train loss=0.6934789419174194\n",
      "Epoch 0, batch 191/625, train loss=0.6976094841957092\n",
      "Epoch 0, batch 192/625, train loss=0.7123236656188965\n",
      "Epoch 0, batch 193/625, train loss=0.6870289444923401\n",
      "Epoch 0, batch 194/625, train loss=0.6912126541137695\n",
      "Epoch 0, batch 195/625, train loss=0.6868239641189575\n",
      "Epoch 0, batch 196/625, train loss=0.6959136128425598\n",
      "Epoch 0, batch 197/625, train loss=0.6912606954574585\n",
      "Epoch 0, batch 198/625, train loss=0.6728867888450623\n",
      "Epoch 0, batch 199/625, train loss=0.6755674481391907\n",
      "Epoch 0, batch 200/625, train loss=0.6534237861633301\n",
      "Epoch 0, batch 201/625, train loss=0.7363418340682983\n",
      "Epoch 0, batch 202/625, train loss=0.685523509979248\n",
      "Epoch 0, batch 203/625, train loss=0.7146860957145691\n",
      "Epoch 0, batch 204/625, train loss=0.713918149471283\n",
      "Epoch 0, batch 205/625, train loss=0.6768204569816589\n",
      "Epoch 0, batch 206/625, train loss=0.7328412532806396\n",
      "Epoch 0, batch 207/625, train loss=0.7103400826454163\n",
      "Epoch 0, batch 208/625, train loss=0.6873095631599426\n",
      "Epoch 0, batch 209/625, train loss=0.6916488409042358\n",
      "Epoch 0, batch 210/625, train loss=0.694047212600708\n",
      "Epoch 0, batch 211/625, train loss=0.6905975341796875\n",
      "Epoch 0, batch 212/625, train loss=0.6878555417060852\n",
      "Epoch 0, batch 213/625, train loss=0.6914634108543396\n",
      "Epoch 0, batch 214/625, train loss=0.6570424437522888\n",
      "Epoch 0, batch 215/625, train loss=0.7328539490699768\n",
      "Epoch 0, batch 216/625, train loss=0.7177017331123352\n",
      "Epoch 0, batch 217/625, train loss=0.7399933338165283\n",
      "Epoch 0, batch 218/625, train loss=0.7250086069107056\n",
      "Epoch 0, batch 219/625, train loss=0.6691498756408691\n",
      "Epoch 0, batch 220/625, train loss=0.6923364996910095\n",
      "Epoch 0, batch 221/625, train loss=0.7200793027877808\n",
      "Epoch 0, batch 222/625, train loss=0.7018095850944519\n",
      "Epoch 0, batch 223/625, train loss=0.693278968334198\n",
      "Epoch 0, batch 224/625, train loss=0.6920617818832397\n",
      "Epoch 0, batch 225/625, train loss=0.6876953840255737\n",
      "Epoch 0, batch 226/625, train loss=0.6858881711959839\n",
      "Epoch 0, batch 227/625, train loss=0.7477601170539856\n",
      "Epoch 0, batch 228/625, train loss=0.7096623182296753\n",
      "Epoch 0, batch 229/625, train loss=0.7086642384529114\n",
      "Epoch 0, batch 230/625, train loss=0.6578465104103088\n",
      "Epoch 0, batch 231/625, train loss=0.69234299659729\n",
      "Epoch 0, batch 232/625, train loss=0.6922420263290405\n",
      "Epoch 0, batch 233/625, train loss=0.6920535564422607\n",
      "Epoch 0, batch 234/625, train loss=0.6856883764266968\n",
      "Epoch 0, batch 235/625, train loss=0.6858115196228027\n",
      "Epoch 0, batch 236/625, train loss=0.6916117072105408\n",
      "Epoch 0, batch 237/625, train loss=0.7080113291740417\n",
      "Epoch 0, batch 238/625, train loss=0.6958833932876587\n",
      "Epoch 0, batch 239/625, train loss=0.6839976906776428\n",
      "Epoch 0, batch 240/625, train loss=0.7005389928817749\n",
      "Epoch 0, batch 241/625, train loss=0.7019888758659363\n",
      "Epoch 0, batch 242/625, train loss=0.6934307217597961\n",
      "Epoch 0, batch 243/625, train loss=0.6856841444969177\n",
      "Epoch 0, batch 244/625, train loss=0.6870996952056885\n",
      "Epoch 0, batch 245/625, train loss=0.6920908689498901\n",
      "Epoch 0, batch 246/625, train loss=0.7113535404205322\n",
      "Epoch 0, batch 247/625, train loss=0.6759426593780518\n",
      "Epoch 0, batch 248/625, train loss=0.7199333310127258\n",
      "Epoch 0, batch 249/625, train loss=0.6640604138374329\n",
      "Epoch 0, batch 250/625, train loss=0.7112457752227783\n",
      "Epoch 0, batch 251/625, train loss=0.7578940391540527\n",
      "Epoch 0, batch 252/625, train loss=0.6963082551956177\n",
      "Epoch 0, batch 253/625, train loss=0.7025421261787415\n",
      "Epoch 0, batch 254/625, train loss=0.731916069984436\n",
      "Epoch 0, batch 255/625, train loss=0.6876466870307922\n",
      "Epoch 0, batch 256/625, train loss=0.6902501583099365\n",
      "Epoch 0, batch 257/625, train loss=0.6909582018852234\n",
      "Epoch 0, batch 258/625, train loss=0.6913265585899353\n",
      "Epoch 0, batch 259/625, train loss=0.7204470038414001\n",
      "Epoch 0, batch 260/625, train loss=0.7076917886734009\n",
      "Epoch 0, batch 261/625, train loss=0.6914150714874268\n",
      "Epoch 0, batch 262/625, train loss=0.6963523626327515\n",
      "Epoch 0, batch 263/625, train loss=0.7139872312545776\n",
      "Epoch 0, batch 264/625, train loss=0.6975187659263611\n",
      "Epoch 0, batch 265/625, train loss=0.6889713406562805\n",
      "Epoch 0, batch 266/625, train loss=0.6931823492050171\n",
      "Epoch 0, batch 267/625, train loss=0.6913114786148071\n",
      "Epoch 0, batch 268/625, train loss=0.6915966272354126\n",
      "Epoch 0, batch 269/625, train loss=0.6988039612770081\n",
      "Epoch 0, batch 270/625, train loss=0.6864016652107239\n",
      "Epoch 0, batch 271/625, train loss=0.6917833685874939\n",
      "Epoch 0, batch 272/625, train loss=0.7273039817810059\n",
      "Epoch 0, batch 273/625, train loss=0.6988337635993958\n",
      "Epoch 0, batch 274/625, train loss=0.6857237219810486\n",
      "Epoch 0, batch 275/625, train loss=0.680388331413269\n",
      "Epoch 0, batch 276/625, train loss=0.6969515085220337\n",
      "Epoch 0, batch 277/625, train loss=0.6812312006950378\n",
      "Epoch 0, batch 278/625, train loss=0.6913560032844543\n",
      "Epoch 0, batch 279/625, train loss=0.7062188982963562\n",
      "Epoch 0, batch 280/625, train loss=0.6827161312103271\n",
      "Epoch 0, batch 281/625, train loss=0.6951635479927063\n",
      "Epoch 0, batch 282/625, train loss=0.6876915693283081\n",
      "Epoch 0, batch 283/625, train loss=0.6945632696151733\n",
      "Epoch 0, batch 284/625, train loss=0.6913027763366699\n",
      "Epoch 0, batch 285/625, train loss=0.6968091130256653\n",
      "Epoch 0, batch 286/625, train loss=0.7023113369941711\n",
      "Epoch 0, batch 287/625, train loss=0.6910044550895691\n",
      "Epoch 0, batch 288/625, train loss=0.6931470632553101\n",
      "Epoch 0, batch 289/625, train loss=0.6925705075263977\n",
      "Epoch 0, batch 290/625, train loss=0.6961327195167542\n",
      "Epoch 0, batch 291/625, train loss=0.6997976303100586\n",
      "Epoch 0, batch 292/625, train loss=0.6914334297180176\n",
      "Epoch 0, batch 293/625, train loss=0.6912549734115601\n",
      "Epoch 0, batch 294/625, train loss=0.6932668685913086\n",
      "Epoch 0, batch 295/625, train loss=0.6957387328147888\n",
      "Epoch 0, batch 296/625, train loss=0.692301869392395\n",
      "Epoch 0, batch 297/625, train loss=0.6923545002937317\n",
      "Epoch 0, batch 298/625, train loss=0.6913046836853027\n",
      "Epoch 0, batch 299/625, train loss=0.6807769536972046\n",
      "Epoch 0, batch 300/625, train loss=0.6946841478347778\n",
      "Epoch 0, batch 301/625, train loss=0.6810933947563171\n",
      "Epoch 0, batch 302/625, train loss=0.6993770003318787\n",
      "Epoch 0, batch 303/625, train loss=0.7101830244064331\n",
      "Epoch 0, batch 304/625, train loss=0.747281551361084\n",
      "Epoch 0, batch 305/625, train loss=0.7327672839164734\n",
      "Epoch 0, batch 306/625, train loss=0.6916925311088562\n",
      "Epoch 0, batch 307/625, train loss=0.6872193813323975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, batch 308/625, train loss=0.6817317008972168\n",
      "Epoch 0, batch 309/625, train loss=0.6917080879211426\n",
      "Epoch 0, batch 310/625, train loss=0.6904004216194153\n",
      "Epoch 0, batch 311/625, train loss=0.6993986368179321\n",
      "Epoch 0, batch 312/625, train loss=0.6925867795944214\n",
      "Epoch 0, batch 313/625, train loss=0.6930519342422485\n",
      "Epoch 0, batch 314/625, train loss=0.6892719864845276\n",
      "Epoch 0, batch 315/625, train loss=0.7036823034286499\n",
      "Epoch 0, batch 316/625, train loss=0.6872789263725281\n",
      "Epoch 0, batch 317/625, train loss=0.7058111429214478\n",
      "Epoch 0, batch 318/625, train loss=0.6863223910331726\n",
      "Epoch 0, batch 319/625, train loss=0.7128068208694458\n",
      "Epoch 0, batch 320/625, train loss=0.7103201746940613\n",
      "Epoch 0, batch 321/625, train loss=0.698043167591095\n",
      "Epoch 0, batch 322/625, train loss=0.6862731575965881\n",
      "Epoch 0, batch 323/625, train loss=0.692210853099823\n",
      "Epoch 0, batch 324/625, train loss=0.6965720653533936\n",
      "Epoch 0, batch 325/625, train loss=0.6988438963890076\n",
      "Epoch 0, batch 326/625, train loss=0.6887272596359253\n",
      "Epoch 0, batch 327/625, train loss=0.6914433836936951\n",
      "Epoch 0, batch 328/625, train loss=0.6809819340705872\n",
      "Epoch 0, batch 329/625, train loss=0.7009721994400024\n",
      "Epoch 0, batch 330/625, train loss=0.685914933681488\n",
      "Epoch 0, batch 331/625, train loss=0.7051224708557129\n",
      "Epoch 0, batch 332/625, train loss=0.7328673005104065\n",
      "Epoch 0, batch 333/625, train loss=0.6802458763122559\n",
      "Epoch 0, batch 334/625, train loss=0.6813843846321106\n",
      "Epoch 0, batch 335/625, train loss=0.68658447265625\n",
      "Epoch 0, batch 336/625, train loss=0.7049707174301147\n",
      "Epoch 0, batch 337/625, train loss=0.6950443983078003\n",
      "Epoch 0, batch 338/625, train loss=0.6973060369491577\n",
      "Epoch 0, batch 339/625, train loss=0.6936060190200806\n",
      "Epoch 0, batch 340/625, train loss=0.6932284832000732\n",
      "Epoch 0, batch 341/625, train loss=0.6933835744857788\n",
      "Epoch 0, batch 342/625, train loss=0.687579333782196\n",
      "Epoch 0, batch 343/625, train loss=0.6886585354804993\n",
      "Epoch 0, batch 344/625, train loss=0.6958074569702148\n",
      "Epoch 0, batch 345/625, train loss=0.6917421221733093\n",
      "Epoch 0, batch 346/625, train loss=0.6567156910896301\n",
      "Epoch 0, batch 347/625, train loss=0.7035504579544067\n",
      "Epoch 0, batch 348/625, train loss=0.7578086256980896\n",
      "Epoch 0, batch 349/625, train loss=0.6661026477813721\n",
      "Epoch 0, batch 350/625, train loss=0.7444947957992554\n",
      "Epoch 0, batch 351/625, train loss=0.7106878757476807\n",
      "Epoch 0, batch 352/625, train loss=0.7053452730178833\n",
      "Epoch 0, batch 353/625, train loss=0.7045261859893799\n",
      "Epoch 0, batch 354/625, train loss=0.6991531252861023\n",
      "Epoch 0, batch 355/625, train loss=0.6944601535797119\n",
      "Epoch 0, batch 356/625, train loss=0.691205620765686\n",
      "Epoch 0, batch 357/625, train loss=0.709069013595581\n",
      "Epoch 0, batch 358/625, train loss=0.6853832602500916\n",
      "Epoch 0, batch 359/625, train loss=0.7352553606033325\n",
      "Epoch 0, batch 360/625, train loss=0.6525253057479858\n",
      "Epoch 0, batch 361/625, train loss=0.68543940782547\n",
      "Epoch 0, batch 362/625, train loss=0.7043619155883789\n",
      "Epoch 0, batch 363/625, train loss=0.7231152057647705\n",
      "Epoch 0, batch 364/625, train loss=0.7109031677246094\n",
      "Epoch 0, batch 365/625, train loss=0.7422280311584473\n",
      "Epoch 0, batch 366/625, train loss=0.7041345834732056\n",
      "Epoch 0, batch 367/625, train loss=0.6968619227409363\n",
      "Epoch 0, batch 368/625, train loss=0.6938140392303467\n",
      "Epoch 0, batch 369/625, train loss=0.7125582695007324\n",
      "Epoch 0, batch 370/625, train loss=0.6711289286613464\n",
      "Epoch 0, batch 371/625, train loss=0.6947494745254517\n",
      "Epoch 0, batch 372/625, train loss=0.6215721368789673\n",
      "Epoch 0, batch 373/625, train loss=0.6757074594497681\n",
      "Epoch 0, batch 374/625, train loss=0.7231780886650085\n",
      "Epoch 0, batch 375/625, train loss=0.678145170211792\n",
      "Epoch 0, batch 376/625, train loss=0.6444525718688965\n",
      "Epoch 0, batch 377/625, train loss=0.6803058385848999\n",
      "Epoch 0, batch 378/625, train loss=0.7181721925735474\n",
      "Epoch 0, batch 379/625, train loss=0.5891684889793396\n",
      "Epoch 0, batch 380/625, train loss=0.755427360534668\n",
      "Epoch 0, batch 381/625, train loss=0.7687963843345642\n",
      "Epoch 0, batch 382/625, train loss=0.7247975468635559\n",
      "Epoch 0, batch 383/625, train loss=0.715355634689331\n",
      "Epoch 0, batch 384/625, train loss=0.7269344329833984\n",
      "Epoch 0, batch 385/625, train loss=0.6987577676773071\n",
      "Epoch 0, batch 386/625, train loss=0.6973851919174194\n",
      "Epoch 0, batch 387/625, train loss=0.6947476863861084\n",
      "Epoch 0, batch 388/625, train loss=0.6912720799446106\n",
      "Epoch 0, batch 389/625, train loss=0.6860141754150391\n",
      "Epoch 0, batch 390/625, train loss=0.6853155493736267\n",
      "Epoch 0, batch 391/625, train loss=0.7248697876930237\n",
      "Epoch 0, batch 392/625, train loss=0.7291702628135681\n",
      "Epoch 0, batch 393/625, train loss=0.6862738132476807\n",
      "Epoch 0, batch 394/625, train loss=0.717298150062561\n",
      "Epoch 0, batch 395/625, train loss=0.7519112229347229\n",
      "Epoch 0, batch 396/625, train loss=0.692550778388977\n",
      "Epoch 0, batch 397/625, train loss=0.6814287900924683\n",
      "Epoch 0, batch 398/625, train loss=0.6846855878829956\n",
      "Epoch 0, batch 399/625, train loss=0.6871823668479919\n",
      "Epoch 0, batch 400/625, train loss=0.6985846161842346\n",
      "Epoch 0, batch 401/625, train loss=0.6943196058273315\n",
      "Epoch 0, batch 402/625, train loss=0.6915112733840942\n",
      "Epoch 0, batch 403/625, train loss=0.6989496946334839\n",
      "Epoch 0, batch 404/625, train loss=0.680855929851532\n",
      "Epoch 0, batch 405/625, train loss=0.7011575102806091\n",
      "Epoch 0, batch 406/625, train loss=0.7205876708030701\n",
      "Epoch 0, batch 407/625, train loss=0.6915133595466614\n",
      "Epoch 0, batch 408/625, train loss=0.7016907334327698\n",
      "Epoch 0, batch 409/625, train loss=0.6868394613265991\n",
      "Epoch 0, batch 410/625, train loss=0.7027633190155029\n",
      "Epoch 0, batch 411/625, train loss=0.6913512349128723\n",
      "Epoch 0, batch 412/625, train loss=0.6936073899269104\n",
      "Epoch 0, batch 413/625, train loss=0.6962391138076782\n",
      "Epoch 0, batch 414/625, train loss=0.6931748390197754\n",
      "Epoch 0, batch 415/625, train loss=0.6952866315841675\n",
      "Epoch 0, batch 416/625, train loss=0.6861668825149536\n",
      "Epoch 0, batch 417/625, train loss=0.6950492262840271\n",
      "Epoch 0, batch 418/625, train loss=0.6865148544311523\n",
      "Epoch 0, batch 419/625, train loss=0.6916208267211914\n",
      "Epoch 0, batch 420/625, train loss=0.6790140271186829\n",
      "Epoch 0, batch 421/625, train loss=0.677853524684906\n",
      "Epoch 0, batch 422/625, train loss=0.7110066413879395\n",
      "Epoch 0, batch 423/625, train loss=0.6854454278945923\n",
      "Epoch 0, batch 424/625, train loss=0.6308324337005615\n",
      "Epoch 0, batch 425/625, train loss=0.7070055603981018\n",
      "Epoch 0, batch 426/625, train loss=0.7087974548339844\n",
      "Epoch 0, batch 427/625, train loss=0.708886444568634\n",
      "Epoch 0, batch 428/625, train loss=0.6968361139297485\n",
      "Epoch 0, batch 429/625, train loss=0.6955403685569763\n",
      "Epoch 0, batch 430/625, train loss=0.7203323841094971\n",
      "Epoch 0, batch 431/625, train loss=0.6923788785934448\n",
      "Epoch 0, batch 432/625, train loss=0.6809748411178589\n",
      "Epoch 0, batch 433/625, train loss=0.7070555090904236\n",
      "Epoch 0, batch 434/625, train loss=0.6957359313964844\n",
      "Epoch 0, batch 435/625, train loss=0.6931747198104858\n",
      "Epoch 0, batch 436/625, train loss=0.6976343393325806\n",
      "Epoch 0, batch 437/625, train loss=0.7011798620223999\n",
      "Epoch 0, batch 438/625, train loss=0.7267053723335266\n",
      "Epoch 0, batch 439/625, train loss=0.6968666911125183\n",
      "Epoch 0, batch 440/625, train loss=0.6866756677627563\n",
      "Epoch 0, batch 441/625, train loss=0.6893548965454102\n",
      "Epoch 0, batch 442/625, train loss=0.6892385482788086\n",
      "Epoch 0, batch 443/625, train loss=0.6938039660453796\n",
      "Epoch 0, batch 444/625, train loss=0.6824846863746643\n",
      "Epoch 0, batch 445/625, train loss=0.7163019776344299\n",
      "Epoch 0, batch 446/625, train loss=0.6870875954627991\n",
      "Epoch 0, batch 447/625, train loss=0.6954313516616821\n",
      "Epoch 0, batch 448/625, train loss=0.6953126788139343\n",
      "Epoch 0, batch 449/625, train loss=0.6988049149513245\n",
      "Epoch 0, batch 450/625, train loss=0.678719162940979\n",
      "Epoch 0, batch 451/625, train loss=0.7012922763824463\n",
      "Epoch 0, batch 452/625, train loss=0.6823444366455078\n",
      "Epoch 0, batch 453/625, train loss=0.6879584193229675\n",
      "Epoch 0, batch 454/625, train loss=0.6987475156784058\n",
      "Epoch 0, batch 455/625, train loss=0.6988560557365417\n",
      "Epoch 0, batch 456/625, train loss=0.6877251267433167\n",
      "Epoch 0, batch 457/625, train loss=0.6776413917541504\n",
      "Epoch 0, batch 458/625, train loss=0.6911981701850891\n",
      "Epoch 0, batch 459/625, train loss=0.7150212526321411\n",
      "Epoch 0, batch 460/625, train loss=0.7083889842033386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, batch 461/625, train loss=0.6881989240646362\n",
      "Epoch 0, batch 462/625, train loss=0.7004512548446655\n",
      "Epoch 0, batch 463/625, train loss=0.6947242021560669\n",
      "Epoch 0, batch 464/625, train loss=0.6942763328552246\n",
      "Epoch 0, batch 465/625, train loss=0.6915109753608704\n",
      "Epoch 0, batch 466/625, train loss=0.6912045478820801\n",
      "Epoch 0, batch 467/625, train loss=0.7198827862739563\n",
      "Epoch 0, batch 468/625, train loss=0.6866614818572998\n",
      "Epoch 0, batch 469/625, train loss=0.6866626739501953\n",
      "Epoch 0, batch 470/625, train loss=0.6675612330436707\n",
      "Epoch 0, batch 471/625, train loss=0.7090970873832703\n",
      "Epoch 0, batch 472/625, train loss=0.6857061982154846\n",
      "Epoch 0, batch 473/625, train loss=0.7048378586769104\n",
      "Epoch 0, batch 474/625, train loss=0.667069137096405\n",
      "Epoch 0, batch 475/625, train loss=0.6788381338119507\n",
      "Epoch 0, batch 476/625, train loss=0.6853504776954651\n",
      "Epoch 0, batch 477/625, train loss=0.7408403158187866\n",
      "Epoch 0, batch 478/625, train loss=0.7138084173202515\n",
      "Epoch 0, batch 479/625, train loss=0.6859495639801025\n",
      "Epoch 0, batch 480/625, train loss=0.7043310403823853\n",
      "Epoch 0, batch 481/625, train loss=0.6913984417915344\n",
      "Epoch 0, batch 482/625, train loss=0.6978955864906311\n",
      "Epoch 0, batch 483/625, train loss=0.6964079141616821\n",
      "Epoch 0, batch 484/625, train loss=0.6891272664070129\n",
      "Epoch 0, batch 485/625, train loss=0.7103655338287354\n",
      "Epoch 0, batch 486/625, train loss=0.7079081535339355\n",
      "Epoch 0, batch 487/625, train loss=0.6875132918357849\n",
      "Epoch 0, batch 488/625, train loss=0.6912204027175903\n",
      "Epoch 0, batch 489/625, train loss=0.7140130400657654\n",
      "Epoch 0, batch 490/625, train loss=0.693534255027771\n",
      "Epoch 0, batch 491/625, train loss=0.6922928690910339\n",
      "Epoch 0, batch 492/625, train loss=0.6936184763908386\n",
      "Epoch 0, batch 493/625, train loss=0.6968031525611877\n",
      "Epoch 0, batch 494/625, train loss=0.6922529935836792\n",
      "Epoch 0, batch 495/625, train loss=0.6936147212982178\n",
      "Epoch 0, batch 496/625, train loss=0.692736029624939\n",
      "Epoch 0, batch 497/625, train loss=0.6920328140258789\n",
      "Epoch 0, batch 498/625, train loss=0.692204475402832\n",
      "Epoch 0, batch 499/625, train loss=0.6917178630828857\n",
      "Epoch 0, batch 500/625, train loss=0.6833123564720154\n",
      "Epoch 0, batch 501/625, train loss=0.6790135502815247\n",
      "Epoch 0, batch 502/625, train loss=0.679943323135376\n",
      "Epoch 0, batch 503/625, train loss=0.7397983074188232\n",
      "Epoch 0, batch 504/625, train loss=0.7015938758850098\n",
      "Epoch 0, batch 505/625, train loss=0.709435224533081\n",
      "Epoch 0, batch 506/625, train loss=0.7000448107719421\n",
      "Epoch 0, batch 507/625, train loss=0.6855977177619934\n",
      "Epoch 0, batch 508/625, train loss=0.7136408686637878\n",
      "Epoch 0, batch 509/625, train loss=0.6872755885124207\n",
      "Epoch 0, batch 510/625, train loss=0.7073432207107544\n",
      "Epoch 0, batch 511/625, train loss=0.6948215365409851\n",
      "Epoch 0, batch 512/625, train loss=0.705939769744873\n",
      "Epoch 0, batch 513/625, train loss=0.7065982222557068\n",
      "Epoch 0, batch 514/625, train loss=0.704318106174469\n",
      "Epoch 0, batch 515/625, train loss=0.6954215168952942\n",
      "Epoch 0, batch 516/625, train loss=0.6834574937820435\n",
      "Epoch 0, batch 517/625, train loss=0.6754233837127686\n",
      "Epoch 0, batch 518/625, train loss=0.715216875076294\n",
      "Epoch 0, batch 519/625, train loss=0.7047975063323975\n",
      "Epoch 0, batch 520/625, train loss=0.7058367729187012\n",
      "Epoch 0, batch 521/625, train loss=0.6958306431770325\n",
      "Epoch 0, batch 522/625, train loss=0.6931695342063904\n",
      "Epoch 0, batch 523/625, train loss=0.6978276371955872\n",
      "Epoch 0, batch 524/625, train loss=0.6968746781349182\n",
      "Epoch 0, batch 525/625, train loss=0.6935189366340637\n",
      "Epoch 0, batch 526/625, train loss=0.6902170777320862\n",
      "Epoch 0, batch 527/625, train loss=0.6936009526252747\n",
      "Epoch 0, batch 528/625, train loss=0.6916547417640686\n",
      "Epoch 0, batch 529/625, train loss=0.7004187703132629\n",
      "Epoch 0, batch 530/625, train loss=0.6901149749755859\n",
      "Epoch 0, batch 531/625, train loss=0.690365731716156\n",
      "Epoch 0, batch 532/625, train loss=0.6952353119850159\n",
      "Epoch 0, batch 533/625, train loss=0.6950505971908569\n",
      "Epoch 0, batch 534/625, train loss=0.6909487247467041\n",
      "Epoch 0, batch 535/625, train loss=0.6933212280273438\n",
      "Epoch 0, batch 536/625, train loss=0.6943833231925964\n",
      "Epoch 0, batch 537/625, train loss=0.6916981339454651\n",
      "Epoch 0, batch 538/625, train loss=0.6932340264320374\n",
      "Epoch 0, batch 539/625, train loss=0.6915663480758667\n",
      "Epoch 0, batch 540/625, train loss=0.6885512471199036\n",
      "Epoch 0, batch 541/625, train loss=0.7005038261413574\n",
      "Epoch 0, batch 542/625, train loss=0.6939330101013184\n",
      "Epoch 0, batch 543/625, train loss=0.6965576410293579\n",
      "Epoch 0, batch 544/625, train loss=0.6915154457092285\n",
      "Epoch 0, batch 545/625, train loss=0.6915767192840576\n",
      "Epoch 0, batch 546/625, train loss=0.6852035522460938\n",
      "Epoch 0, batch 547/625, train loss=0.6913610696792603\n",
      "Epoch 0, batch 548/625, train loss=0.6912281513214111\n",
      "Epoch 0, batch 549/625, train loss=0.6832546591758728\n",
      "Epoch 0, batch 550/625, train loss=0.6961900591850281\n",
      "Epoch 0, batch 551/625, train loss=0.6751500964164734\n",
      "Epoch 0, batch 552/625, train loss=0.7049514651298523\n",
      "Epoch 0, batch 553/625, train loss=0.699151337146759\n",
      "Epoch 0, batch 554/625, train loss=0.7334569096565247\n",
      "Epoch 0, batch 555/625, train loss=0.7081445455551147\n",
      "Epoch 0, batch 556/625, train loss=0.6985528469085693\n",
      "Epoch 0, batch 557/625, train loss=0.6918640732765198\n",
      "Epoch 0, batch 558/625, train loss=0.6927850842475891\n",
      "Epoch 0, batch 559/625, train loss=0.6983379125595093\n",
      "Epoch 0, batch 560/625, train loss=0.6875075697898865\n",
      "Epoch 0, batch 561/625, train loss=0.675868570804596\n",
      "Epoch 0, batch 562/625, train loss=0.6924298405647278\n",
      "Epoch 0, batch 563/625, train loss=0.7109846472740173\n",
      "Epoch 0, batch 564/625, train loss=0.7318918108940125\n",
      "Epoch 0, batch 565/625, train loss=0.7209059596061707\n",
      "Epoch 0, batch 566/625, train loss=0.7236493825912476\n",
      "Epoch 0, batch 567/625, train loss=0.6859492063522339\n",
      "Epoch 0, batch 568/625, train loss=0.6873254179954529\n",
      "Epoch 0, batch 569/625, train loss=0.6864665150642395\n",
      "Epoch 0, batch 570/625, train loss=0.7003712058067322\n",
      "Epoch 0, batch 571/625, train loss=0.6930077075958252\n",
      "Epoch 0, batch 572/625, train loss=0.6911317706108093\n",
      "Epoch 0, batch 573/625, train loss=0.6939601898193359\n",
      "Epoch 0, batch 574/625, train loss=0.6911993622779846\n",
      "Epoch 0, batch 575/625, train loss=0.6723095178604126\n",
      "Epoch 0, batch 576/625, train loss=0.6727127432823181\n",
      "Epoch 0, batch 577/625, train loss=0.7021535038948059\n",
      "Epoch 0, batch 578/625, train loss=0.6857660412788391\n",
      "Epoch 0, batch 579/625, train loss=0.7081705927848816\n",
      "Epoch 0, batch 580/625, train loss=0.7207513451576233\n",
      "Epoch 0, batch 581/625, train loss=0.6865189671516418\n",
      "Epoch 0, batch 582/625, train loss=0.7071046233177185\n",
      "Epoch 0, batch 583/625, train loss=0.685654878616333\n",
      "Epoch 0, batch 584/625, train loss=0.6939135789871216\n",
      "Epoch 0, batch 585/625, train loss=0.6853361129760742\n",
      "Epoch 0, batch 586/625, train loss=0.7049387097358704\n",
      "Epoch 0, batch 587/625, train loss=0.7014707922935486\n",
      "Epoch 0, batch 588/625, train loss=0.701251745223999\n",
      "Epoch 0, batch 589/625, train loss=0.6896414160728455\n",
      "Epoch 0, batch 590/625, train loss=0.6932880282402039\n",
      "Epoch 0, batch 591/625, train loss=0.6909809708595276\n",
      "Epoch 0, batch 592/625, train loss=0.6866106390953064\n",
      "Epoch 0, batch 593/625, train loss=0.6952214241027832\n"
     ]
    }
   ],
   "source": [
    "# training cycle\n",
    "for epoch in range(epoch_num):\n",
    "    train(epoch)\n",
    "    validate(epoch)\n",
    "    if epoch % lr_change:\n",
    "        change_lr()\n",
    "        \n",
    "    # save checkpoint\n",
    "    # visualize training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
